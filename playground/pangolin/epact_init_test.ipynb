{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyKDL as kdl\n",
    "import kdl_parser_py.urdf\n",
    "import pickle\n",
    "import torch\n",
    "import sys, os\n",
    "import cv2 as cv\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import transformations as tf\n",
    "import numpy as np\n",
    "from itertools import repeat\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "util_path = os.path.abspath(\"../utils/\")\n",
    "sys.path.append(util_path)\n",
    "import transformation as trans\n",
    "import projections as proj\n",
    "import robot_visualize as rbvis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/data1/act/train_act_scara_3cam/3cam/config.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m policy_model_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_step_30000_seed_0.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m stats_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_stats.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     15\u001b[0m     policy_config \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy_config\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m     policy_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mep_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data1/act/train_act_scara_3cam/3cam/config.pkl'"
     ]
    }
   ],
   "source": [
    "act_path = \"/home/zfei/code/act-plus-plus/\"\n",
    "sys.path.append(act_path)\n",
    "from policy import EPACTPolicy, ACTPolicy\n",
    "from brl_constants import TASK_CONFIGS\n",
    "ckpt_path = \"/mnt/data1/act/train_act_scara_3cam/3cam_epact_v4_1_lep10/\"\n",
    "\n",
    "config_path = ckpt_path + \"config.pkl\"\n",
    "# step = 500000\n",
    "# policy_model_path = ckpt_path + \"policy_step_%i_seed_0.ckpt\"%step\n",
    "policy_model_path = ckpt_path + \"policy_step_30000_seed_0.ckpt\"\n",
    "\n",
    "stats_path = ckpt_path + \"dataset_stats.pkl\"\n",
    "\n",
    "with open(config_path, \"rb\") as f:\n",
    "    policy_config = pickle.load(f)['policy_config']\n",
    "    policy_config[\"ep_weight\"] = 1.0\n",
    "    print(policy_config)\n",
    "\n",
    "policy = EPACTPolicy(policy_config)\n",
    "# policy = ACTPolicy(policy_config)\n",
    "policy_class = \"EPACT\"\n",
    "# policy_class = \"ACT\"\n",
    "\n",
    "\n",
    "pre_process_qpos = (\n",
    "    lambda s_qpos: torch.from_numpy((s_qpos - stats[\"qpos_mean\"]) / stats[\"qpos_std\"])\n",
    "    .float()\n",
    "    .cuda()\n",
    "    .unsqueeze(0)\n",
    ")\n",
    "post_process_action = lambda a: a * stats[\"action_std\"] + stats[\"action_mean\"]\n",
    "\n",
    "print(\"loading EPACT policy success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wrist', 'wrist_down', 'top']\n"
     ]
    }
   ],
   "source": [
    "camera_names = policy_config[\"camera_names\"]\n",
    "print(camera_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_proccess_img(img):\n",
    "    img_torch = torch.from_numpy(img).unsqueeze(0)\n",
    "    img_torch = torch.einsum(\"k h w c -> k c h w\", img_torch)\n",
    "    img_torch = (img_torch / 255.0).float().cuda().unsqueeze(0)\n",
    "    return img_torch\n",
    "\n",
    "def pre_process_multi_img(curr_images):\n",
    "    # a list of images [w, h, 3]\n",
    "    curr_image = np.stack(curr_images, axis=0)\n",
    "    img_torch = torch.from_numpy(curr_image)\n",
    "    img_torch = torch.einsum(\"k h w c -> k c h w\", img_torch)\n",
    "    img_torch = (img_torch / 255.0).float().cuda().unsqueeze(0)\n",
    "    return img_torch\n",
    "\n",
    "def post_process_all_actions(all_actions):\n",
    "    # post process all actions\n",
    "    joint_states_traj = []\n",
    "    actual_actions = []\n",
    "    for i, raw_action in enumerate(all_actions):\n",
    "        actual_action = post_process_action(raw_action)\n",
    "        actual_actions.append(actual_action)\n",
    "    actual_actions = np.asarray(actual_actions)\n",
    "    # print(actual_actions)\n",
    "    return actual_actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "act_path = \"/home/zfei/code/act-plus-plus/\"\n",
    "sys.path.append(act_path)\n",
    "from utils import EpisodicDataset\n",
    "from utils import find_all_hdf5, flatten_list, get_norm_stats\n",
    "from utils import load_data # data functions\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_pass_epact(data, policy):\n",
    "    image_data, qpos_data, action_data, is_pad, end_pose_data = data\n",
    "    image_data, qpos_data, action_data, is_pad, end_pose_data = image_data.cuda(), qpos_data.cuda(), action_data.cuda(), is_pad.cuda(), end_pose_data.cuda()\n",
    "    return policy(qpos_data, image_data, action_data, is_pad, end_pose_data) # TODO remove None\n",
    "\n",
    "def forward_pass(data, policy):\n",
    "    image_data, qpos_data, action_data, is_pad = data\n",
    "    image_data, qpos_data, action_data, is_pad = image_data.cuda(), qpos_data.cuda(), action_data.cuda(), is_pad.cuda()\n",
    "    return policy(qpos_data, image_data, action_data, is_pad) # TODO remove None\n",
    "\n",
    "def repeater(data_loader):\n",
    "    epoch = 0\n",
    "    for loader in repeat(data_loader):\n",
    "        for data in loader:\n",
    "            yield data\n",
    "        print(f'Epoch {epoch} done')\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 51 hdf5 files\n"
     ]
    }
   ],
   "source": [
    "dataset_path_list = find_all_hdf5(\"/mnt/data1/act/train_act_scara_3cam/data1/\", skip_mirrored_data=False)\n",
    "norm_stats, all_episode_len = get_norm_stats(dataset_path_list)\n",
    "\n",
    "camera_names = ['wrist', 'wrist_down', 'top']\n",
    "train_episode_ids = [0, 1]\n",
    "train_episode_len = [all_episode_len[i] for i in train_episode_ids]\n",
    "\n",
    "val_episode_ids = [0, 1]\n",
    "val_episode_len = [all_episode_len[i] for i in val_episode_ids]\n",
    "\n",
    "chunk_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/mnt/data1/act/train_act_scara_3cam/data1/\"\n",
    "\n",
    "TASK_CONFIGS = {\n",
    "        'epact':{\n",
    "        'dataset_dir': dataset_dir + '/train_act_scara_3cam',\n",
    "        'num_episodes': 100,\n",
    "        'episode_len': 300,\n",
    "        'camera_names': ['wrist', 'wrist_down', 'top']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_config = TASK_CONFIGS['epact']\n",
    "name_filter = task_config.get('name_filter', lambda n: True)\n",
    "batch_size_train = 8\n",
    "batch_size_val = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 51 hdf5 files\n",
      "\n",
      "\n",
      "Data from: ['/mnt/data1/act/train_act_scara_3cam/data1/']\n",
      "- Train on [50] episodes\n",
      "- Test on [1] episodes\n",
      "\n",
      "\n",
      "Found 51 hdf5 files\n",
      "Norm stats from: ['/mnt/data1/act/train_act_scara_3cam/data1/']\n",
      "train_episode_len: [120, 108, 118, 104, 140, 135, 136, 177, 129, 117, 168, 135, 144, 175, 125, 106, 111, 132, 121, 103, 154, 124, 153, 137, 168, 128, 101, 110, 124, 110, 128, 161, 151, 127, 116, 135, 114, 104, 118, 123, 90, 107, 118, 139, 117, 116, 98, 136, 165, 121], val_episode_len: [154], train_episode_ids: [36  5 11  3 50 31 17 44  9 48 19 29 45 25 18 22 12 15  2 30 10 28 41  0\n",
      " 34 16 27 24 23 26  4 37 32  1 42  8 20 47 13 43 21  6 33 38 35 39  7 14\n",
      " 49 46], val_episode_ids: [40]\n",
      "val data:  /mnt/data1/act/train_act_scara_3cam/data1/episode_9.hdf5\n",
      "URDF Path:  /home/zfei/code/act-plus-plus/urdf/hitbot_model.urdf\n",
      "kdl_parse urdf ok?:  True\n",
      "augment_images:  False\n",
      "Initializing transformations\n",
      "URDF Path:  /home/zfei/code/act-plus-plus/urdf/hitbot_model.urdf\n",
      "kdl_parse urdf ok?:  True\n",
      "augment_images:  False\n",
      "Initializing transformations\n",
      "Augment images: False, train_num_workers: 2, val_num_workers: 2\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 100\n",
    "skip_mirrored_data = False\n",
    "load_pretrain = False\n",
    "train_ratio = 0.99\n",
    "train_dataloader,val_dataloader, stats, _ = load_data(dataset_dir, \n",
    "                                                       name_filter, \n",
    "                                                       camera_names, \n",
    "                                                       batch_size_train, \n",
    "                                                       batch_size_val, \n",
    "                                                       chunk_size,\n",
    "                                                       skip_mirrored_data,\n",
    "                                                       load_pretrain,\n",
    "                                                       policy_class,\n",
    "                                                       stats_dir_l=None,\n",
    "                                                       sample_weights=None,\n",
    "                                                       train_ratio=train_ratio)\n",
    "train_dataloader = repeater(train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': tensor(0.4697, device='cuda:0', grad_fn=<MeanBackward0>),\n",
       " 'kl': tensor(9.8818, device='cuda:0', grad_fn=<SelectBackward0>),\n",
       " 'ep': tensor(0.1555, device='cuda:0', grad_fn=<MeanBackward0>),\n",
       " 'loss': tensor(99.4427, device='cuda:0', grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(train_dataloader)\n",
    "forward_pass_epact(data, policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.kl_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_data torch.Size([8, 3, 3, 480, 640])\n",
      "qpos_data torch.Size([8, 4])\n",
      "action_data torch.Size([8, 100, 5])\n",
      "is_pad torch.Size([8, 100])\n"
     ]
    }
   ],
   "source": [
    "print(\"image_data\", image_data.shape)\n",
    "print(\"qpos_data\", qpos_data.shape)\n",
    "print(\"action_data\", action_data.shape)\n",
    "print(\"is_pad\", is_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.model.num_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image torch.Size([8, 3, 3, 480, 640])\n",
      "qpos torch.Size([8, 4])\n",
      "actions torch.Size([8, 100, 5])\n",
      "is_pad torch.Size([8, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l1': tensor(0.7795, device='cuda:0', grad_fn=<MeanBackward0>),\n",
       " 'kl': tensor(7.9838, device='cuda:0', grad_fn=<SelectBackward0>),\n",
       " 'loss': tensor(80.6176, device='cuda:0', grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy(qpos_data, image_data, action_data, is_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_actions:  (100, 5)\n",
      "action_obs_np:  (127, 5)\n"
     ]
    }
   ],
   "source": [
    "h5data_file = \"/mnt/data1/act/train_act_scara_3cam/data1/episode_30.hdf5\"\n",
    "\n",
    "with torch.inference_mode():\n",
    "    with h5py.File(h5data_file, \"r\") as root:\n",
    "        index = 0\n",
    "        qpos_np = root[\"/observations/qpos\"][index]\n",
    "        img_wrist_up_np = root[\"/observations/images/wrist\"][index]\n",
    "        img_wrist_down_np = root[\"/observations/images/wrist_down\"][index]\n",
    "        img_top_np = root[\"/observations/images/top\"][index]\n",
    "\n",
    "        qpos_all =  root[\"/observations/qpos\"][()]\n",
    "\n",
    "        curr_images = [img_wrist_up_np, img_wrist_down_np, img_top_np]\n",
    "\n",
    "        action_obs_np = root[\"action\"][()]\n",
    "        qpos = pre_process_qpos(qpos_np)\n",
    "        curr_images_torch = pre_process_multi_img(curr_images)\n",
    "        # print(\"qpos: \",qpos.shape)\n",
    "        all_actions, all_end_poses = policy(qpos, curr_images_torch)\n",
    "        all_actions = all_actions.squeeze(0).cpu().numpy()\n",
    "\n",
    "        print(\"all_actions: \", all_actions.shape)\n",
    "        print(\"action_obs_np: \", action_obs_np.shape)\n",
    "\n",
    "        # actual_action = post_process_action(raw_action)\n",
    "        # print(\"actual action %i\"%index, actual_action)\n",
    "\n",
    "\n",
    "action_obs_np = action_obs_np[index:]\n",
    "qpos_all = np.asarray(qpos_all)\n",
    "qpos_all = qpos_all[index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
