{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import PyKDL as kdl\n",
    "import kdl_parser_py.urdf\n",
    "import pickle\n",
    "import torch\n",
    "import sys, os\n",
    "import cv2 as cv\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import transformations as tf\n",
    "import numpy as np\n",
    "from itertools import repeat\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "util_path = os.path.abspath(\"../utils/\")\n",
    "sys.path.append(util_path)\n",
    "import transformation as trans\n",
    "import projections as proj\n",
    "import robot_visualize as rbvis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 1e-05, 'num_queries': 100, 'kl_weight': 10, 'hidden_dim': 512, 'dim_feedforward': 3200, 'lr_backbone': 1e-05, 'backbone': 'resnet18', 'enc_layers': 4, 'dec_layers': 7, 'nheads': 8, 'camera_names': ['wrist', 'wrist_down', 'top'], 'vq': False, 'vq_class': None, 'vq_dim': None, 'action_dim': 5, 'state_dim': 4, 'no_encoder': False, 'ep_weight': 1.0}\n",
      "EPACT Args:  Namespace(lr=1e-05, lr_backbone=1e-05, batch_size=2, weight_decay=0.0001, epochs=300, lr_drop=200, clip_max_norm=0.1, backbone='resnet18', dilation=False, position_embedding='sine', camera_names=['wrist', 'wrist_down', 'top'], enc_layers=4, dec_layers=7, dim_feedforward=3200, hidden_dim=512, dropout=0.1, nheads=8, num_queries=100, pre_norm=False, masks=False, eval=False, onscreen_render=False, ckpt_dir='/mnt/data1/act/act_demo_z1_push_red/ckpt', policy_class='EPACT', task_name='act_demo_z1_push_red', seed=0, num_steps=2000, kl_weight=10, chunk_size=None, temporal_agg=False, use_vq=False, vq=False, vq_class=None, vq_dim=None, load_pretrain=False, action_dim=5, eval_every=500, validate_every=500, save_every=500, resume_ckpt_path=None, no_encoder=False, skip_mirrored_data=False, actuator_network_dir=None, history_len=None, future_len=None, prediction_len=None, state_dim=4, ep_weight=1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zfei/anaconda3/envs/py310/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/zfei/anaconda3/envs/py310/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use end_pose_to_action:  True\n",
      "Use VQ: False, None, None\n",
      "number of parameters: 106.24M\n",
      "EPACTPolicy ep_weight 1.0\n",
      "KL Weight 10\n",
      "loading EPACT policy success\n"
     ]
    }
   ],
   "source": [
    "act_path = \"/home/zfei/code/act-plus-plus/\"\n",
    "sys.path.append(act_path)\n",
    "from policy import EPACTPolicy, ACTPolicy\n",
    "from brl_constants import TASK_CONFIGS\n",
    "ckpt_path = \"/mnt/data1/act/train_act_scara_3cam/3cam/\"\n",
    "\n",
    "config_path = ckpt_path + \"config.pkl\"\n",
    "# step = 500000\n",
    "# policy_model_path = ckpt_path + \"policy_step_%i_seed_0.ckpt\"%step\n",
    "policy_model_path = ckpt_path + \"policy_step_60000_seed_0.ckpt\"\n",
    "\n",
    "stats_path = ckpt_path + \"dataset_stats.pkl\"\n",
    "\n",
    "with open(config_path, \"rb\") as f:\n",
    "    policy_config = pickle.load(f)['policy_config']\n",
    "    policy_config[\"ep_weight\"] = 1.0\n",
    "    print(policy_config)\n",
    "\n",
    "policy = EPACTPolicy(policy_config)\n",
    "# policy = ACTPolicy(policy_config)\n",
    "policy_class = \"EPACT\"\n",
    "# policy_class = \"ACT\"\n",
    "\n",
    "with open(stats_path, \"rb\") as f:\n",
    "    stats = pickle.load(f)\n",
    "\n",
    "pre_process_qpos = (\n",
    "    lambda s_qpos: torch.from_numpy((s_qpos - stats[\"qpos_mean\"]) / stats[\"qpos_std\"])\n",
    "    .float()\n",
    "    .cuda()\n",
    "    .unsqueeze(0)\n",
    ")\n",
    "post_process_action = lambda a: a * stats[\"action_std\"] + stats[\"action_mean\"]\n",
    "\n",
    "print(\"loading EPACT policy success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wrist', 'wrist_down', 'top']\n"
     ]
    }
   ],
   "source": [
    "camera_names = policy_config[\"camera_names\"]\n",
    "print(camera_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_proccess_img(img):\n",
    "    img_torch = torch.from_numpy(img).unsqueeze(0)\n",
    "    img_torch = torch.einsum(\"k h w c -> k c h w\", img_torch)\n",
    "    img_torch = (img_torch / 255.0).float().cuda().unsqueeze(0)\n",
    "    return img_torch\n",
    "\n",
    "def pre_process_multi_img(curr_images):\n",
    "    # a list of images [w, h, 3]\n",
    "    curr_image = np.stack(curr_images, axis=0)\n",
    "    img_torch = torch.from_numpy(curr_image)\n",
    "    img_torch = torch.einsum(\"k h w c -> k c h w\", img_torch)\n",
    "    img_torch = (img_torch / 255.0).float().cuda().unsqueeze(0)\n",
    "    return img_torch\n",
    "\n",
    "def post_process_all_actions(all_actions):\n",
    "    # post process all actions\n",
    "    joint_states_traj = []\n",
    "    actual_actions = []\n",
    "    for i, raw_action in enumerate(all_actions):\n",
    "        actual_action = post_process_action(raw_action)\n",
    "        actual_actions.append(actual_action)\n",
    "    actual_actions = np.asarray(actual_actions)\n",
    "    # print(actual_actions)\n",
    "    return actual_actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "act_path = \"/home/zfei/code/act-plus-plus/\"\n",
    "sys.path.append(act_path)\n",
    "from utils import EpisodicDataset\n",
    "from utils import find_all_hdf5, flatten_list, get_norm_stats\n",
    "from utils import load_data # data functions\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_pass_epact(data, policy):\n",
    "    image_data, qpos_data, action_data, is_pad, end_pose_data = data\n",
    "    image_data, qpos_data, action_data, is_pad, end_pose_data = image_data.cuda(), qpos_data.cuda(), action_data.cuda(), is_pad.cuda(), end_pose_data.cuda()\n",
    "    return policy(qpos_data, image_data, action_data, is_pad, end_pose_data) # TODO remove None\n",
    "\n",
    "def forward_pass(data, policy):\n",
    "    image_data, qpos_data, action_data, is_pad = data\n",
    "    image_data, qpos_data, action_data, is_pad = image_data.cuda(), qpos_data.cuda(), action_data.cuda(), is_pad.cuda()\n",
    "    return policy(qpos_data, image_data, action_data, is_pad) # TODO remove None\n",
    "\n",
    "def repeater(data_loader):\n",
    "    epoch = 0\n",
    "    for loader in repeat(data_loader):\n",
    "        for data in loader:\n",
    "            yield data\n",
    "        print(f'Epoch {epoch} done')\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 51 hdf5 files\n"
     ]
    }
   ],
   "source": [
    "dataset_path_list = find_all_hdf5(\"/mnt/data1/act/train_act_scara_3cam/data1/\", skip_mirrored_data=False)\n",
    "norm_stats, all_episode_len = get_norm_stats(dataset_path_list)\n",
    "\n",
    "camera_names = ['wrist', 'wrist_down', 'top']\n",
    "train_episode_ids = [0, 1]\n",
    "train_episode_len = [all_episode_len[i] for i in train_episode_ids]\n",
    "\n",
    "val_episode_ids = [0, 1]\n",
    "val_episode_len = [all_episode_len[i] for i in val_episode_ids]\n",
    "\n",
    "chunk_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/mnt/data1/act/train_act_scara_3cam/data1/\"\n",
    "\n",
    "TASK_CONFIGS = {\n",
    "        'epact':{\n",
    "        'dataset_dir': dataset_dir + '/train_act_scara_3cam',\n",
    "        'num_episodes': 100,\n",
    "        'episode_len': 300,\n",
    "        'camera_names': ['wrist', 'wrist_down', 'top']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_config = TASK_CONFIGS['epact']\n",
    "name_filter = task_config.get('name_filter', lambda n: True)\n",
    "batch_size_train = 8\n",
    "batch_size_val = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 51 hdf5 files\n",
      "\n",
      "\n",
      "Data from: ['/mnt/data1/act/train_act_scara_3cam/data1/']\n",
      "- Train on [50] episodes\n",
      "- Test on [1] episodes\n",
      "\n",
      "\n",
      "Found 51 hdf5 files\n",
      "Norm stats from: ['/mnt/data1/act/train_act_scara_3cam/data1/']\n",
      "train_episode_len: [127, 124, 111, 168, 103, 137, 108, 121, 110, 117, 107, 125, 139, 121, 153, 135, 144, 118, 128, 110, 132, 165, 114, 104, 136, 154, 117, 140, 101, 118, 168, 129, 177, 175, 154, 123, 104, 90, 98, 151, 128, 118, 106, 120, 116, 124, 135, 116, 136, 161], val_episode_len: [135], train_episode_ids: [ 1 28 12 34 30  0  5  2 26 35  6 18 38 46 41 29 45 11 16 24 15 49 20  3\n",
      " 14 10 48 50 27 13 19  9 44 25 40 43 47 21  7 32  4 33 22 36 39 23  8 42\n",
      " 17 37], val_episode_ids: [31]\n",
      "val data:  /mnt/data1/act/train_act_scara_3cam/data1/episode_28.hdf5\n",
      "URDF Path:  /home/zfei/code/act-plus-plus/urdf/hitbot_model.urdf\n",
      "kdl_parse urdf ok?:  True\n",
      "augment_images:  False\n",
      "Initializing transformations\n",
      "URDF Path:  /home/zfei/code/act-plus-plus/urdf/hitbot_model.urdf\n",
      "kdl_parse urdf ok?:  True\n",
      "augment_images:  False\n",
      "Initializing transformations\n",
      "Augment images: False, train_num_workers: 2, val_num_workers: 2\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 100\n",
    "skip_mirrored_data = False\n",
    "load_pretrain = False\n",
    "train_ratio = 0.99\n",
    "train_dataloader,val_dataloader, stats, _ = load_data(dataset_dir, \n",
    "                                                       name_filter, \n",
    "                                                       camera_names, \n",
    "                                                       batch_size_train, \n",
    "                                                       batch_size_val, \n",
    "                                                       chunk_size,\n",
    "                                                       skip_mirrored_data,\n",
    "                                                       load_pretrain,\n",
    "                                                       policy_class,\n",
    "                                                       stats_dir_l=None,\n",
    "                                                       sample_weights=None,\n",
    "                                                       train_ratio=train_ratio)\n",
    "train_dataloader = repeater(train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(train_dataloader)\n\u001b[0;32m----> 2\u001b[0m image_data, qpos_data, action_data, is_pad \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m      3\u001b[0m image_data, qpos_data, action_data, is_pad, \u001b[38;5;241m=\u001b[39m image_data\u001b[38;5;241m.\u001b[39mcuda(), qpos_data\u001b[38;5;241m.\u001b[39mcuda(), action_data\u001b[38;5;241m.\u001b[39mcuda(), is_pad\u001b[38;5;241m.\u001b[39mcuda()\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "data = next(train_dataloader)\n",
    "forward_pass_epact(data, policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_data torch.Size([8, 3, 3, 480, 640])\n",
      "qpos_data torch.Size([8, 4])\n",
      "action_data torch.Size([8, 100, 5])\n",
      "is_pad torch.Size([8, 100])\n"
     ]
    }
   ],
   "source": [
    "print(\"image_data\", image_data.shape)\n",
    "print(\"qpos_data\", qpos_data.shape)\n",
    "print(\"action_data\", action_data.shape)\n",
    "print(\"is_pad\", is_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.model.num_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image torch.Size([8, 3, 3, 480, 640])\n",
      "qpos torch.Size([8, 4])\n",
      "actions torch.Size([8, 100, 5])\n",
      "is_pad torch.Size([8, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l1': tensor(0.7795, device='cuda:0', grad_fn=<MeanBackward0>),\n",
       " 'kl': tensor(7.9838, device='cuda:0', grad_fn=<SelectBackward0>),\n",
       " 'loss': tensor(80.6176, device='cuda:0', grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy(qpos_data, image_data, action_data, is_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_actions:  (100, 5)\n",
      "action_obs_np:  (127, 5)\n"
     ]
    }
   ],
   "source": [
    "h5data_file = \"/mnt/data1/act/train_act_scara_3cam/data1/episode_30.hdf5\"\n",
    "\n",
    "with torch.inference_mode():\n",
    "    with h5py.File(h5data_file, \"r\") as root:\n",
    "        index = 0\n",
    "        qpos_np = root[\"/observations/qpos\"][index]\n",
    "        img_wrist_up_np = root[\"/observations/images/wrist\"][index]\n",
    "        img_wrist_down_np = root[\"/observations/images/wrist_down\"][index]\n",
    "        img_top_np = root[\"/observations/images/top\"][index]\n",
    "\n",
    "        qpos_all =  root[\"/observations/qpos\"][()]\n",
    "\n",
    "        curr_images = [img_wrist_up_np, img_wrist_down_np, img_top_np]\n",
    "\n",
    "        action_obs_np = root[\"action\"][()]\n",
    "        qpos = pre_process_qpos(qpos_np)\n",
    "        curr_images_torch = pre_process_multi_img(curr_images)\n",
    "        # print(\"qpos: \",qpos.shape)\n",
    "        all_actions, all_end_poses = policy(qpos, curr_images_torch)\n",
    "        all_actions = all_actions.squeeze(0).cpu().numpy()\n",
    "\n",
    "        print(\"all_actions: \", all_actions.shape)\n",
    "        print(\"action_obs_np: \", action_obs_np.shape)\n",
    "\n",
    "        # actual_action = post_process_action(raw_action)\n",
    "        # print(\"actual action %i\"%index, actual_action)\n",
    "\n",
    "\n",
    "action_obs_np = action_obs_np[index:]\n",
    "qpos_all = np.asarray(qpos_all)\n",
    "qpos_all = qpos_all[index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
