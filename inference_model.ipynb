{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from itertools import repeat\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "import wandb\n",
    "import time\n",
    "from torchvision import transforms\n",
    "import h5py\n",
    "import cv2 as cv\n",
    "from brl_constants import FPS\n",
    "from brl_constants import PUPPET_GRIPPER_JOINT_OPEN\n",
    "from brl_constants import TASK_CONFIGS\n",
    "from utils import load_data  # data functions\n",
    "from utils import sample_box_pose, sample_insertion_pose  # robot functions\n",
    "from utils import (\n",
    "    compute_dict_mean,\n",
    "    set_seed,\n",
    "    detach_dict,\n",
    "    calibrate_linear_vel,\n",
    "    postprocess_base_action,\n",
    ")  # helper functions\n",
    "from policy import ACTPolicy, CNNMLPPolicy\n",
    "\n",
    "# from policy import ACTPolicy, CNNMLPPolicy, DiffusionPolicy\n",
    "from visualize_episodes import save_videos\n",
    "\n",
    "from detr.models.latent_model import Latent_Model_Transformer\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"act_demo_z1_push_red\"\n",
    "task_config = TASK_CONFIGS[task_name]\n",
    "camera_names = task_config[\"camera_names\"]\n",
    "\n",
    "ckpt_dir = \"/mnt/data1/act/act_demo_z1_push_red/ckpt\"\n",
    "policy_class = \"ACT\"\n",
    "kl_weight = 10\n",
    "chunk_size = 100\n",
    "hidden_dim = 512\n",
    "batch_size = 8\n",
    "dim_feedforward = 3200\n",
    "num_steps = 2000\n",
    "lr = 1e-5\n",
    "lr_backbone = 1e-5\n",
    "seed = 0\n",
    "backbone = \"resnet18\"\n",
    "state_dim = 6\n",
    "enc_layers = 4\n",
    "dec_layers = 7\n",
    "nheads = 8\n",
    "\n",
    "policy_config = {\n",
    "    \"lr\": lr,\n",
    "    \"num_queries\": chunk_size,\n",
    "    \"kl_weight\": kl_weight,\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"dim_feedforward\": dim_feedforward,\n",
    "    \"lr_backbone\": lr_backbone,\n",
    "    \"backbone\": backbone,\n",
    "    \"enc_layers\": enc_layers,\n",
    "    \"dec_layers\": dec_layers,\n",
    "    \"nheads\": nheads,\n",
    "    \"camera_names\": camera_names,\n",
    "    \"vq\": False,\n",
    "    \"vq_class\": None,\n",
    "    \"vq_dim\": None,\n",
    "    \"action_dim\": 6,\n",
    "    \"no_encoder\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser(\"Set transformer detector\", add_help=False)\n",
    "    parser.add_argument(\"--lr\", default=1e-4, type=float)  # will be overridden\n",
    "    parser.add_argument(\"--lr_backbone\", default=1e-5, type=float)  # will be overridden\n",
    "    parser.add_argument(\"--batch_size\", default=2, type=int)  # not used\n",
    "    parser.add_argument(\"--weight_decay\", default=1e-4, type=float)\n",
    "    parser.add_argument(\"--epochs\", default=300, type=int)  # not used\n",
    "    parser.add_argument(\"--lr_drop\", default=200, type=int)  # not used\n",
    "    parser.add_argument(\n",
    "        \"--clip_max_norm\",\n",
    "        default=0.1,\n",
    "        type=float,  # not used\n",
    "        help=\"gradient clipping max norm\",\n",
    "    )\n",
    "\n",
    "    # Model parameters\n",
    "    # * Backbone\n",
    "    parser.add_argument(\n",
    "        \"--backbone\",\n",
    "        default=\"resnet18\",\n",
    "        type=str,  # will be overridden\n",
    "        help=\"Name of the convolutional backbone to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dilation\",\n",
    "        action=\"store_true\",\n",
    "        help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--position_embedding\",\n",
    "        default=\"sine\",\n",
    "        type=str,\n",
    "        choices=(\"sine\", \"learned\"),\n",
    "        help=\"Type of positional embedding to use on top of the image features\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--camera_names\",\n",
    "        default=[],\n",
    "        type=list,  # will be overridden\n",
    "        help=\"A list of camera names\",\n",
    "    )\n",
    "\n",
    "    # * Transformer\n",
    "    parser.add_argument(\n",
    "        \"--enc_layers\",\n",
    "        default=4,\n",
    "        type=int,  # will be overridden\n",
    "        help=\"Number of encoding layers in the transformer\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dec_layers\",\n",
    "        default=6,\n",
    "        type=int,  # will be overridden\n",
    "        help=\"Number of decoding layers in the transformer\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dim_feedforward\",\n",
    "        default=2048,\n",
    "        type=int,  # will be overridden\n",
    "        help=\"Intermediate size of the feedforward layers in the transformer blocks\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--hidden_dim\",\n",
    "        default=256,\n",
    "        type=int,  # will be overridden\n",
    "        help=\"Size of the embeddings (dimension of the transformer)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dropout\", default=0.1, type=float, help=\"Dropout applied in the transformer\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nheads\",\n",
    "        default=8,\n",
    "        type=int,  # will be overridden\n",
    "        help=\"Number of attention heads inside the transformer's attentions\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_queries\",\n",
    "        default=400,\n",
    "        type=int,  # will be overridden\n",
    "        help=\"Number of query slots\",\n",
    "    )\n",
    "    parser.add_argument(\"--pre_norm\", action=\"store_true\")\n",
    "\n",
    "    # * Segmentation\n",
    "    parser.add_argument(\n",
    "        \"--masks\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Train segmentation head if the flag is provided\",\n",
    "    )\n",
    "\n",
    "    # repeat args in imitate_episodes just to avoid error. Will not be used\n",
    "    parser.add_argument(\"--eval\", action=\"store_true\")\n",
    "    parser.add_argument(\"--onscreen_render\", action=\"store_true\")\n",
    "    parser.add_argument(\n",
    "        \"--ckpt_dir\", action=\"store\", type=str, help=\"ckpt_dir\", required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--policy_class\",\n",
    "        action=\"store\",\n",
    "        type=str,\n",
    "        help=\"policy_class, capitalize\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--task_name\", action=\"store\", type=str, help=\"task_name\", required=False\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", action=\"store\", type=int, help=\"seed\", required=False)\n",
    "    parser.add_argument(\n",
    "        \"--num_steps\", action=\"store\", type=int, help=\"num_epochs\", required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--kl_weight\", action=\"store\", type=int, help=\"KL Weight\", required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--chunk_size\", action=\"store\", type=int, help=\"chunk_size\", required=False\n",
    "    )\n",
    "    parser.add_argument(\"--temporal_agg\", action=\"store_true\")\n",
    "\n",
    "    parser.add_argument(\"--use_vq\", action=\"store_true\")\n",
    "    parser.add_argument(\n",
    "        \"--vq_class\", action=\"store\", type=int, help=\"vq_class\", required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--vq_dim\", action=\"store\", type=int, help=\"vq_dim\", required=False\n",
    "    )\n",
    "    parser.add_argument(\"--load_pretrain\", action=\"store_true\", default=False)\n",
    "    parser.add_argument(\"--action_dim\", action=\"store\", type=int, required=False)\n",
    "    parser.add_argument(\n",
    "        \"--eval_every\",\n",
    "        action=\"store\",\n",
    "        type=int,\n",
    "        default=500,\n",
    "        help=\"eval_every\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--validate_every\",\n",
    "        action=\"store\",\n",
    "        type=int,\n",
    "        default=500,\n",
    "        help=\"validate_every\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_every\",\n",
    "        action=\"store\",\n",
    "        type=int,\n",
    "        default=500,\n",
    "        help=\"save_every\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume_ckpt_path\",\n",
    "        action=\"store\",\n",
    "        type=str,\n",
    "        help=\"load_ckpt_path\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\"--no_encoder\", action=\"store_true\")\n",
    "    parser.add_argument(\"--skip_mirrored_data\", action=\"store_true\")\n",
    "    parser.add_argument(\n",
    "        \"--actuator_network_dir\",\n",
    "        action=\"store\",\n",
    "        type=str,\n",
    "        help=\"actuator_network_dir\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\"--history_len\", action=\"store\", type=int)\n",
    "    parser.add_argument(\"--future_len\", action=\"store\", type=int)\n",
    "    parser.add_argument(\"--prediction_len\", action=\"store\", type=int)\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    \"DETR training and evaluation script\", parents=[get_args_parser()]\n",
    ")\n",
    "args = parser.parse_args([\"--policy_class\", \"ACT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_steps': 2000,\n",
       " 'eval_every': 500,\n",
       " 'validate_every': 500,\n",
       " 'save_every': 500,\n",
       " 'ckpt_dir': '/mnt/data1/act/act_demo_z1_push_red/ckpt',\n",
       " 'resume_ckpt_path': None,\n",
       " 'episode_len': 400,\n",
       " 'state_dim': 6,\n",
       " 'lr': 1e-05,\n",
       " 'policy_class': 'ACT',\n",
       " 'onscreen_render': False,\n",
       " 'policy_config': {'lr': 1e-05,\n",
       "  'num_queries': 100,\n",
       "  'kl_weight': 10,\n",
       "  'hidden_dim': 512,\n",
       "  'dim_feedforward': 3200,\n",
       "  'lr_backbone': 1e-05,\n",
       "  'backbone': 'resnet18',\n",
       "  'enc_layers': 4,\n",
       "  'dec_layers': 7,\n",
       "  'nheads': 8,\n",
       "  'camera_names': ['wrist'],\n",
       "  'vq': False,\n",
       "  'vq_class': None,\n",
       "  'vq_dim': None,\n",
       "  'action_dim': 6,\n",
       "  'no_encoder': False},\n",
       " 'task_name': 'act_demo_z1_push_red',\n",
       " 'seed': 0,\n",
       " 'temporal_agg': False,\n",
       " 'camera_names': ['wrist'],\n",
       " 'real_robot': True,\n",
       " 'load_pretrain': False,\n",
       " 'actuator_config': {'actuator_network_dir': None,\n",
       "  'history_len': None,\n",
       "  'future_len': None,\n",
       "  'prediction_len': None}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_path = \"/mnt/data1/act/act_demo_z1_push_red/ckpt/config.pkl\"\n",
    "with open(config_path, \"rb\") as f:\n",
    "    policy_config = pickle.load(f)\n",
    "policy_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_config = {\n",
    "    \"lr\": 1e-05,\n",
    "    \"num_queries\": 100,\n",
    "    \"kl_weight\": 10,\n",
    "    \"hidden_dim\": 512,\n",
    "    \"dim_feedforward\": 3200,\n",
    "    \"lr_backbone\": 1e-05,\n",
    "    \"backbone\": \"resnet18\",\n",
    "    \"enc_layers\": 4,\n",
    "    \"dec_layers\": 7,\n",
    "    \"nheads\": 8,\n",
    "    \"camera_names\": [\"wrist\"],\n",
    "    \"vq\": False,\n",
    "    \"vq_class\": None,\n",
    "    \"vq_dim\": None,\n",
    "    \"action_dim\": 6,\n",
    "    \"no_encoder\": False,\n",
    "    \"task_name\": \"act_demo_z1_push_red\",\n",
    "    \"ckpt_dir\": \"/mnt/data1/act/act_demo_z1_push_red/ckpt/policy_best.ckpt\",\n",
    "    \"num_steps\": 2000,\n",
    "    \"lr\": 1e-5,\n",
    "    \"seed\": 0,\n",
    "    \"policy_class\": \"ACT\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETR Args:  Namespace(lr=0.0001, lr_backbone=1e-05, batch_size=2, weight_decay=0.0001, epochs=300, lr_drop=200, clip_max_norm=0.1, backbone='resnet18', dilation=False, position_embedding='sine', camera_names=[], enc_layers=4, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=400, pre_norm=False, masks=False, eval=False, onscreen_render=False, ckpt_dir='/mnt/data1/act/act_demo_z1_push_red/ckpt', policy_class='ACT', task_name='act_demo_z1_push_red', seed=0, num_steps=2000, kl_weight=None, chunk_size=None, temporal_agg=False, use_vq=False, vq_class=None, vq_dim=None, load_pretrain=False, action_dim=None, eval_every=500, validate_every=500, save_every=500, resume_ckpt_path=None, no_encoder=False, skip_mirrored_data=False, actuator_network_dir=None, history_len=None, future_len=None, prediction_len=None)\n",
      "ACT Args:  Namespace(lr=1e-05, lr_backbone=1e-05, batch_size=2, weight_decay=0.0001, epochs=300, lr_drop=200, clip_max_norm=0.1, backbone='resnet18', dilation=False, position_embedding='sine', camera_names=['wrist'], enc_layers=4, dec_layers=7, dim_feedforward=3200, hidden_dim=512, dropout=0.1, nheads=8, num_queries=100, pre_norm=False, masks=False, eval=False, onscreen_render=False, ckpt_dir='/mnt/data1/act/act_demo_z1_push_red/ckpt/policy_best.ckpt', policy_class='ACT', task_name='act_demo_z1_push_red', seed=0, num_steps=2000, kl_weight=10, chunk_size=None, temporal_agg=False, use_vq=False, vq_class=None, vq_dim=None, load_pretrain=False, action_dim=6, eval_every=500, validate_every=500, save_every=500, resume_ckpt_path=None, no_encoder=False, skip_mirrored_data=False, actuator_network_dir=None, history_len=None, future_len=None, prediction_len=None, vq=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zfei/anaconda3/envs/torch/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/zfei/anaconda3/envs/torch/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use VQ: False, None, None\n",
      "number of parameters: 83.91M\n",
      "KL Weight 10\n",
      "<All keys matched successfully>\n",
      "Loaded:  /mnt/data1/act/act_demo_z1_push_red/ckpt/policy_best.ckpt\n"
     ]
    }
   ],
   "source": [
    "# ckpt_path = \"/mnt/data1/act/act_demo_z1_push_red/ckpt/policy_best.ckpt\"\n",
    "\n",
    "policy = ACTPolicy(policy_config)\n",
    "loading_status = policy.deserialize(torch.load(policy_config[\"ckpt_dir\"]))\n",
    "print(loading_status)\n",
    "policy.cuda()\n",
    "policy.eval()\n",
    "print(\"Loaded: \", policy_config[\"ckpt_dir\"])\n",
    "stats_path = \"/mnt/data1/act/act_demo_z1_push_red/ckpt/dataset_stats.pkl\"\n",
    "with open(stats_path, \"rb\") as f:\n",
    "    stats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process_qpos = (\n",
    "    lambda s_qpos: torch.from_numpy((s_qpos - stats[\"qpos_mean\"]) / stats[\"qpos_std\"])\n",
    "    .float()\n",
    "    .cuda()\n",
    "    .unsqueeze(0)\n",
    ")\n",
    "post_process_action = lambda a: a * stats[\"action_std\"] + stats[\"action_mean\"]\n",
    "query_frequency = policy_config[\"num_queries\"]\n",
    "BASE_DELAY = 13\n",
    "query_frequency -= BASE_DELAY\n",
    "max_timesteps = int(400)  # may increase for real-world tasks\n",
    "\n",
    "\n",
    "def pre_proccess_img(img):\n",
    "    img_torch = torch.from_numpy(img).unsqueeze(0)\n",
    "    img_torch = torch.einsum(\"k h w c -> k c h w\", img_torch)\n",
    "    img_torch = (img_torch / 255.0).float().cuda().unsqueeze(0)\n",
    "    return img_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'curr_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m culmulated_delay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_timesteps):\n\u001b[0;32m----> 6\u001b[0m     all_actions \u001b[38;5;241m=\u001b[39m policy(qpos, \u001b[43mcurr_image\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'curr_image' is not defined"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    time0 = time.time()\n",
    "    DT = 1 / FPS\n",
    "    culmulated_delay = 0\n",
    "    for t in range(max_timesteps):\n",
    "        all_actions = policy(qpos, curr_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ave inference time: 0.008824615478515626\n",
      "ave fps: 113.31442555046989\n"
     ]
    }
   ],
   "source": [
    "h5data_file = \"/mnt/data1/act/act_demo_z1_push_red/episode_50.hdf5\"\n",
    "with torch.inference_mode():\n",
    "    with h5py.File(h5data_file, \"r\") as root:\n",
    "        tic = time.time()\n",
    "        for index in range(100):\n",
    "            qpos_np = root[\"/observations/qpos\"][index]\n",
    "            img_np = root[\"/observations/images/wrist\"][index]\n",
    "            qpos = pre_process_qpos(qpos_np)\n",
    "            curr_img = pre_proccess_img(img_np)\n",
    "            # print(\"qpos: \",qpos.shape)\n",
    "            # print(\"curr_img: \",curr_img.shape)\n",
    "            all_actions = policy(qpos, curr_img)\n",
    "            # print(\"all_actions: \", all_actions.shape)\n",
    "        print(\"ave inference time:\", (time.time()-tic)/100)\n",
    "        print(\"ave fps:\", 100/(time.time()-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual action 0 [-0.27048644  0.28409767 -1.3577642   1.0764028   0.26963568  0.00364211]\n",
      "actual action 1 [-0.27048752  0.28410238 -1.357761    1.0763901   0.2696367   0.00364215]\n",
      "actual action 2 [-0.2705621   0.2836768  -1.3582773   1.0780164   0.26963714  0.00363334]\n",
      "actual action 3 [-0.27061105  0.2891059  -1.3531578   1.0689373   0.27101272  0.00433012]\n",
      "actual action 4 [-0.27016604  0.31722903 -1.3378539   1.0374017   0.27179506  0.00484861]\n",
      "actual action 5 [-0.27019146  0.31718796 -1.3378454   1.0374027   0.27181014  0.0048494 ]\n",
      "actual action 6 [-0.27219585  0.3273734  -1.3273525   1.0072116   0.27330372  0.00488721]\n",
      "actual action 7 [-0.27810234  0.35023183 -1.3025774   0.9570044   0.27953658  0.00487137]\n",
      "actual action 8 [-0.281964    0.36514717 -1.2856654   0.92124915  0.28307167  0.00488848]\n",
      "actual action 9 [-0.28510687  0.3823722  -1.269188    0.88054186  0.28775796  0.00490073]\n",
      "actual action 10 [-0.2853763   0.38249648 -1.2684586   0.87936634  0.28799316  0.00490519]\n",
      "actual action 11 [-0.28929773  0.40040982 -1.2498293   0.8417674   0.29225773  0.0049445 ]\n",
      "actual action 12 [-0.29602438  0.4204098  -1.2261221   0.79141486  0.30025417  0.00483518]\n",
      "actual action 13 [-0.29650217  0.4208935  -1.2248503   0.789078    0.3006928   0.00483594]\n",
      "actual action 14 [-0.30701506  0.47714803 -1.1695752   0.6700178   0.31184888  0.00496202]\n",
      "actual action 15 [-0.3084663   0.47893313 -1.1653872   0.6638512   0.31311348  0.00494202]\n",
      "actual action 16 [-0.30727857  0.47796607 -1.1634668   0.6468204   0.31423143  0.00489881]\n",
      "actual action 17 [-0.31682697  0.52467394 -1.122036    0.5641761   0.32170194  0.00472114]\n",
      "actual action 18 [-0.3178098   0.5259832  -1.1191113   0.559445    0.32254273  0.00471001]\n",
      "actual action 19 [-0.32731545  0.59022564 -1.0564473   0.43400192  0.33321562  0.00437248]\n",
      "actual action 20 [-0.33761114  0.62854093 -1.0097632   0.33082852  0.3439711   0.00374377]\n",
      "actual action 21 [-0.33903843  0.6307361  -1.0055146   0.32510838  0.34534514  0.00371204]\n",
      "actual action 22 [-0.3368852   0.6208334  -1.0177925   0.35758963  0.3437972   0.00405638]\n",
      "actual action 23 [-0.34296748  0.7422339  -0.9361991   0.1646908   0.35128394  0.00320629]\n",
      "actual action 24 [-0.3449277   0.7438386  -0.9306677   0.15833372  0.35322165  0.00315381]\n",
      "actual action 25 [-0.35097435  0.67054534 -0.9555414   0.26208392  0.3558335   0.00387911]\n",
      "actual action 26 [-0.35688576  0.7157526  -0.92411536  0.15754429  0.36033985  0.00281602]\n",
      "actual action 27 [-0.3597396   0.8306568  -0.855007   -0.00215293  0.36660868  0.0026907 ]\n",
      "actual action 28 [-0.37607822  0.80192816 -0.8319624  -0.00486283  0.37636745  0.00247176]\n",
      "actual action 29 [-0.38009015  0.80640364 -0.819994   -0.02105965  0.37997225  0.00230835]\n",
      "actual action 30 [-0.3791885   0.86853755 -0.7928108  -0.0995827   0.38151392  0.00208671]\n",
      "actual action 31 [-0.38594717  0.90676725 -0.7570456  -0.16970354  0.38903797  0.00187192]\n",
      "actual action 32 [-0.39113832  0.8372404  -0.77673167 -0.08932801  0.390376    0.00149083]\n",
      "actual action 33 [-0.39520472  0.84860957 -0.759994   -0.11556034  0.39364353  0.00128926]\n",
      "actual action 34 [-0.39984378  0.9036134  -0.71331596 -0.20606121  0.40000057  0.00102569]\n"
     ]
    }
   ],
   "source": [
    "h5data_file = \"/mnt/data1/act/act_demo_z1_push_red/episode_50.hdf5\"\n",
    "with torch.inference_mode():\n",
    "    with h5py.File(h5data_file, \"r\") as root:\n",
    "        tic = time.time()\n",
    "        for index in range(100):\n",
    "            qpos_np = root[\"/observations/qpos\"][index]\n",
    "            img_np = root[\"/observations/images/wrist\"][index]\n",
    "            qpos = pre_process_qpos(qpos_np)\n",
    "            curr_img = pre_proccess_img(img_np)\n",
    "            # print(\"qpos: \",qpos.shape)\n",
    "            # print(\"curr_img: \",curr_img.shape)\n",
    "            all_actions = policy(qpos, curr_img)\n",
    "            # print(\"all_actions: \", all_actions.shape)\n",
    "\n",
    "            raw_action = all_actions[:, 0]\n",
    "            raw_action = raw_action.squeeze(0).cpu().numpy()\n",
    "            actual_action = post_process_action(raw_action)\n",
    "            print(\"actual action %i\"%index, actual_action)\n",
    "\n",
    "        print(\"ave inference time:\", (time.time()-tic)/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 480, 640])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6369, -1.5069, -1.7223,  1.7077, -0.6148,  0.0204]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_process(qpos_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    time0 = time.time()\n",
    "    DT = 1 / FPS\n",
    "    culmulated_delay = 0\n",
    "    all_actions = policy(qpos, curr_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
