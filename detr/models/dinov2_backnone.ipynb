{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.dinov2.dinov2 import DINOv2\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import sys, os\n",
    "from torch import nn\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "    'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder='vits'\n",
    "\n",
    "intermediate_layer_idx = {\n",
    "'vits': [2, 5, 8, 11],\n",
    "'vitb': [2, 5, 8, 11], \n",
    "'vitl': [4, 11, 17, 23], \n",
    "'vitg': [9, 19, 29, 39]\n",
    "}\n",
    "\n",
    "dino_backbone = DINOv2(model_name=encoder).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 490, 644, 3)\n"
     ]
    }
   ],
   "source": [
    "# image = np.random.rand(518, 518, 3)\n",
    "image = np.random.rand(490, 644, 3)\n",
    "\n",
    "image = image.astype('float32')/255\n",
    "images_arr = np.expand_dims(image, axis=0)\n",
    "print(images_arr.shape)\n",
    "input_tensor = torch.Tensor(np.transpose(images_arr, [0, 3, 2, 1])).to(DEVICE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_norm_clstoken:  (1, 384)\n",
      "x_norm_regtokens:  (1, 0, 384)\n",
      "x_norm_patchtokens:  (1, 1610, 384)\n",
      "x_prenorm:  (1, 1611, 384)\n",
      "ret:  {'x_norm_clstoken': tensor([[ 0.4053, -2.4967, -0.3243, -1.8044, -0.6511,  0.0798,  2.2217, -0.6900,\n",
      "         -0.4979, -0.0444, -0.8815, -0.4863, -0.2996,  0.1270,  1.2281, -1.9720,\n",
      "         -0.8986,  0.7999,  0.1510, -1.3546, -1.0754, -1.2510, -0.8343, -0.8719,\n",
      "         -0.7795, -0.1154,  0.0317,  0.1778,  1.1575,  0.1968,  1.1554,  0.5081,\n",
      "         -0.3818, -0.0678, -1.6300,  0.9168,  0.9229,  0.0439, -0.5499,  1.2073,\n",
      "          0.7164, -0.4153,  1.6352, -0.6176,  0.9814, -0.1579,  0.3655,  0.0869,\n",
      "         -1.3129,  0.2041, -0.8349, -0.1806,  0.9245, -0.0254, -0.7807,  0.0168,\n",
      "         -0.6424, -1.5370,  1.5536,  0.1421,  0.4377,  0.3102, -0.2822,  0.4147,\n",
      "         -2.7648,  0.1212,  0.2114,  1.9111,  1.5413,  0.0321, -1.6190,  0.8200,\n",
      "         -0.1481,  0.8727,  0.5020, -0.4007,  1.3255,  0.4614,  0.7335, -0.0741,\n",
      "          0.7543, -0.9212,  0.9231,  0.6398, -0.3811, -0.8879,  0.6520, -1.9340,\n",
      "         -1.4764, -1.3275, -0.0722, -1.8917, -1.3079,  0.3605, -0.6110, -0.0393,\n",
      "          0.7262,  0.8780, -1.2674,  0.5001,  1.0744,  0.4013, -1.1477, -0.1386,\n",
      "         -1.7960,  1.0743, -0.1542,  1.1083,  0.1946, -0.6343, -1.0515, -1.0786,\n",
      "          1.4838, -1.1599, -0.0113, -0.0836,  0.0218, -1.0432,  0.2809, -0.0187,\n",
      "         -0.6019, -1.5200,  0.0965,  0.3194, -0.2444,  0.2733,  0.1141, -0.2103,\n",
      "          0.7798,  1.6002, -0.4440, -0.4425, -1.4481,  0.0663, -1.8749, -0.6595,\n",
      "         -1.1938,  0.9404,  1.8022, -0.8305,  1.7265,  0.0540,  0.2434,  0.3866,\n",
      "          0.0538,  1.2167,  0.2430,  0.9006,  0.3559,  0.0217, -1.9366, -0.7154,\n",
      "          0.1144,  1.4807, -3.0329, -0.6656, -1.1486, -0.2714, -0.0413, -0.0780,\n",
      "         -1.5222, -0.4529, -1.5685, -0.9129,  1.3602,  0.5342, -1.6720,  2.1089,\n",
      "         -0.5808,  0.0124,  0.4791,  0.5843, -0.4664,  0.1361, -1.3933, -0.6619,\n",
      "          1.5015, -0.2750,  0.2061,  0.3563,  1.7281,  0.2384, -0.9121, -0.6159,\n",
      "          1.3289, -0.1051,  0.5188, -0.0765,  1.7067,  1.0198,  0.7992, -0.4972,\n",
      "         -1.6483, -0.6211, -0.6105,  1.5104,  0.7301, -0.0809, -0.0286, -1.1287,\n",
      "          1.5932, -0.4554,  0.7277,  1.5100,  0.8766, -0.0165,  0.2091,  1.0271,\n",
      "          0.2431,  0.2254, -1.6991, -1.4725,  0.1758,  1.3036, -0.4347,  2.5005,\n",
      "         -0.8887, -0.9297,  1.2892, -1.0708, -0.6174,  0.1476,  0.3749,  1.3371,\n",
      "          0.9468, -0.1384, -0.3381,  0.9308,  1.6162,  0.3910, -0.8835, -1.3115,\n",
      "         -0.8364, -1.1745,  0.1945,  2.5848, -0.7495, -0.9111,  0.0337,  0.5895,\n",
      "          1.3847, -1.0165, -0.3237, -0.8462, -0.1339,  0.3618,  0.5333,  0.5791,\n",
      "         -0.8613, -1.1607, -0.8088, -0.4806,  0.5574,  1.2731,  0.5989, -1.4142,\n",
      "          1.6241, -0.3867, -1.1152,  1.2742, -0.1834,  0.2096,  0.4087,  1.3372,\n",
      "          0.3531, -0.1097,  0.0915, -0.5750,  0.0983, -0.9572, -0.3307,  1.9553,\n",
      "          0.3995, -1.1948, -0.6462, -0.5553,  2.3411, -0.4733,  0.8769,  0.8603,\n",
      "         -0.0470,  0.0213,  0.3543,  0.9598,  1.8206, -0.0515, -0.8954, -0.8506,\n",
      "         -0.7078, -0.8264,  0.9346, -0.6669,  0.1104, -0.3016, -0.8985, -0.2393,\n",
      "         -1.0078,  1.3854, -0.6482, -1.4679, -0.2475,  0.1847,  0.1763,  1.6633,\n",
      "         -0.0890,  0.4334, -0.2059,  0.8005, -0.6608,  1.5334,  0.5436,  1.4047,\n",
      "          0.9775,  0.5227,  0.9686,  0.3398,  1.1521,  0.8837, -0.2823,  1.3659,\n",
      "         -1.6777,  0.1893, -0.7909, -1.2158,  1.1204, -0.2166, -1.3526,  0.3574,\n",
      "          0.4654, -0.2860, -1.2001, -0.4517, -1.1601,  0.7233,  0.6403,  0.7094,\n",
      "         -0.4246,  1.5932,  0.7356, -0.8656,  0.5511, -1.5511,  1.4215,  1.4867,\n",
      "          0.4590, -1.0667, -0.8915,  0.2679, -0.4160, -1.1458, -2.9798,  0.5216,\n",
      "          2.0451,  0.8754,  0.0125,  0.8834, -0.8249,  0.9625,  0.1172, -1.1439,\n",
      "          1.1733,  1.1552,  0.5659, -2.2508,  0.5170, -1.0990, -0.6241,  2.9606,\n",
      "          0.8964, -0.2150, -0.0051,  0.3133, -1.7854, -0.7228,  0.1944,  1.8771,\n",
      "         -0.2372,  1.2578,  1.3923,  0.3719, -0.6824, -1.4510, -1.4321, -0.5308]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>), 'x_norm_regtokens': tensor([], device='cuda:0', size=(1, 0, 384), grad_fn=<SliceBackward0>), 'x_norm_patchtokens': tensor([[[ 0.2805, -2.5077, -0.0381,  ..., -1.4161, -1.4357, -0.6524],\n",
      "         [ 0.2643, -2.5919, -0.0860,  ..., -1.4056, -1.4949, -0.4275],\n",
      "         [ 0.3073, -2.5885,  0.0905,  ..., -1.3798, -1.4717, -0.5080],\n",
      "         ...,\n",
      "         [ 0.3270, -2.5287, -0.0286,  ..., -1.6111, -1.3657, -0.4464],\n",
      "         [ 0.1711, -2.5070, -0.0153,  ..., -1.6280, -1.4872, -0.5759],\n",
      "         [ 0.2313, -2.4125, -0.0988,  ..., -1.4899, -1.4238, -0.5186]]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>), 'x_prenorm': tensor([[[ 0.3788, -2.1113, -0.2472,  ..., -1.2140, -1.1978, -0.4244],\n",
      "         [ 0.2772, -2.0948,  0.0062,  ..., -1.1662, -1.1828, -0.5165],\n",
      "         [ 0.2593, -2.1369, -0.0346,  ..., -1.1417, -1.2166, -0.3211],\n",
      "         ...,\n",
      "         [ 0.3142, -2.0868,  0.0152,  ..., -1.3154, -1.1090, -0.3361],\n",
      "         [ 0.1858, -2.0876,  0.0275,  ..., -1.3414, -1.2219, -0.4483],\n",
      "         [ 0.2364, -2.0042, -0.0434,  ..., -1.2223, -1.1663, -0.3991]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>), 'masks': None}\n",
      "dino_features:  (1, 384, 1610)\n"
     ]
    }
   ],
   "source": [
    "result = dino_backbone.forward_features(input_tensor)\n",
    "dino_features = result['x_norm_patchtokens'].permute(0, 2, 1).cpu().detach().numpy()\n",
    "print(\"x_norm_clstoken: \", result['x_norm_clstoken'].cpu().detach().numpy().shape)\n",
    "print(\"x_norm_regtokens: \", result['x_norm_regtokens'].cpu().detach().numpy().shape)\n",
    "print(\"x_norm_patchtokens: \", result['x_norm_patchtokens'].cpu().detach().numpy().shape)\n",
    "print(\"x_prenorm: \", result['x_prenorm'].cpu().detach().numpy().shape)\n",
    "\n",
    "ret = dino_backbone(input_tensor)\n",
    "print(\"ret: \", ret)\n",
    "print(\"dino_features: \", dino_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVAE backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backbone import build_backbone, Backbone, Joiner\n",
    "from position_encoding import build_position_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = argparse.Namespace(hidden_dim=384, position_embedding='sine')\n",
    "\n",
    "print(args.hidden_dim)  # Output: 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PositionEmbeddingSine()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embedding = build_position_encoding(args)\n",
    "position_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 384, 1369])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_np = np.random.rand(8, 512, 15, 20)\n",
    "x_np = np.random.rand(8, 384, 1369)\n",
    "\n",
    "x_tensor = torch.Tensor(x_np).to(DEVICE)\n",
    "x_tensor.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pe \u001b[38;5;241m=\u001b[39m \u001b[43mposition_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m pe\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/act-plus-plus/detr/models/position_encoding.py:38\u001b[0m, in \u001b[0;36mPositionEmbeddingSine.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m     36\u001b[0m not_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(x[\u001b[38;5;241m0\u001b[39m, [\u001b[38;5;241m0\u001b[39m]])\n\u001b[1;32m     37\u001b[0m y_embed \u001b[38;5;241m=\u001b[39m not_mask\u001b[38;5;241m.\u001b[39mcumsum(\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 38\u001b[0m x_embed \u001b[38;5;241m=\u001b[39m \u001b[43mnot_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcumsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize:\n\u001b[1;32m     40\u001b[0m     eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "pe = position_embedding(x_tensor)\n",
    "pe.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Joiner(dino_backbone, position_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_backbone = False\n",
    "return_interm_layers = False\n",
    "dilation = False\n",
    "backbone = Backbone('resnet18', train_backbone, return_interm_layers, dilation).to(DEVICE) \n",
    "backbone"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
